{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e304cf9b-5c7b-4733-9279-af2d4a35f524",
   "metadata": {},
   "source": [
    "## Load/Sample Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f3e4326-5e85-4c82-8523-3c1d3bcd73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "from lib.inference import sample_unsloth\n",
    "\n",
    "\n",
    "default_prompt = \"Write a satirical headline in the style of Onion News.\"\n",
    "\n",
    "default_generation_args = {\n",
    "    \"num_return_sequences\": 50,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 20,\n",
    "    \"min_p\": 0\n",
    "}\n",
    "    \n",
    "def get_as_message(prompt):\n",
    "    return [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "def sample_cached(\n",
    "    cache_path,\n",
    "    lora_path,\n",
    "    prompt=default_prompt,\n",
    "    sequences=50,\n",
    "    seed=1337,\n",
    "    is_cot=False,\n",
    "):\n",
    "    try:\n",
    "        with open(cache_path) as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    prompts = {\"prompt\": get_as_message(prompt)}\n",
    "    generation_args = dict(default_generation_args)\n",
    "    generation_args['num_return_sequences'] = sequences\n",
    "    \n",
    "    samples = sample_unsloth(lora_path, prompts=prompts, generation_args=generation_args, seed=seed)[\"prompt\"]\n",
    "\n",
    "    if is_cot:\n",
    "        samples = [sample.split('\\n')[-1] for sample in samples]\n",
    "\n",
    "    with open(cache_path, 'w') as f:\n",
    "        json.dump(samples, f)\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c09babba-0b56-4eb8-89bb-07fed0cc951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.7: Fast Qwen3 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ad9900c8d84d74ad476dfa31cd93ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "baseline = sample_cached(\n",
    "    './test-data/sample-baseline.txt',\n",
    "    'unsloth/Qwen3-4B-Instruct-2507',\n",
    "    # The baseline models often continues on, after the headline.\n",
    "    prompt=\"Write a satirical headline in the style of Onion News. Only respond with the headline.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eea486d2-6cd1-4ca7-b92f-65d2b0aa5e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.7: Fast Qwen3 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d34511b8189e446d9343b0999875699d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.7 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "finetune = sample_cached(\n",
    "    './test-data/sample-finetune.txt',\n",
    "    './finetunes/02-unsloth-finetune/checkpoint-100/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3095e4f-013a-4a47-bd90-e99560b6e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.7: Fast Qwen3 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.9. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = True]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba683ba41bad4ca38cef04238dae2f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cot = sample_cached(\n",
    "    './test-data/sample-cot.txt',\n",
    "    './finetunes/03-unsloth-CoT/checkpoint-60/',\n",
    "    is_cot=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce8947ba-644e-4b00-9737-195ad9eb0379",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test-data/onion-headlines.txt') as f:\n",
    "    golden_samples = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd05f2-d1a9-4c0e-b08b-da306818a2a4",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75570cec-1dd3-47fa-98a2-2ee44f22bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# Make sure you have an API set in your environment variable\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e8c394e-7b19-4b1a-90e5-70243d89bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Define the result model\n",
    "class Preference(enum.Enum):\n",
    "    A = 'A'\n",
    "    B = 'B'\n",
    "\n",
    "class ResponseSchema(BaseModel):\n",
    "    analysis_A: str\n",
    "    analysis_B: str\n",
    "    preference: Preference\n",
    "    reasoning: str\n",
    "\n",
    "# Load the prompt\n",
    "with open('./prompts/evaluation.txt') as f:\n",
    "    eval_prompt = f.read().strip()\n",
    "\n",
    "def get_prompt(headline_a, headline_b):\n",
    "    return eval_prompt.format(headline_a=headline_a, headline_b=headline_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "943f3ab9-c9b0-42d9-97b9-e55192a1462a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResponseSchema(analysis_A='A', analysis_B='B', preference=<Preference.A: 'A'>, reasoning='asdf')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResponseSchema(analysis_A='A', analysis_B='B', preference='A', reasoning='asdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f5ab8be-91c9-413b-a8de-1d0a8415e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import time\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "\n",
    "\n",
    "def compare(xs, ys, output_file, count=10, offset=0, req_per_min=10, dry_run=False):\n",
    "    length = min(len(xs), len(ys))\n",
    "    if count:\n",
    "        length = min(length, count + offset)\n",
    "    \n",
    "    min_seconds = 60 / req_per_min\n",
    "    wins = [0, 0]\n",
    "    results = []\n",
    "    with open(output_file, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow(['Headline_A', 'Headline_B', 'Preference', 'Swapped', 'Prompt', 'analysis_A', 'analysis_B', 'Reasoning'])\n",
    "        last_call = None\n",
    "        for idx in tnrange(offset, length):\n",
    "            x = xs[idx]\n",
    "            y = ys[idx]\n",
    "            swap = random.random() < 0.5\n",
    "            if swap:\n",
    "                x, y = y, x\n",
    "            prompt = get_prompt(x, y)\n",
    "            now = time.monotonic()\n",
    "            if last_call is not None and now < last_call + min_seconds:\n",
    "                time.sleep(last_call + min_seconds - now)\n",
    "            \n",
    "            if dry_run:\n",
    "                output = ResponseSchema(preference=Preference.A, reasoning='XD')\n",
    "            else:\n",
    "                response = client.models.generate_content(\n",
    "                    model=\"gemini-2.5-flash\",\n",
    "                    contents=prompt,\n",
    "                    config={\n",
    "                        'response_mime_type': 'application/json',\n",
    "                        'response_schema': ResponseSchema,\n",
    "                    }\n",
    "                )\n",
    "                output = response.parsed\n",
    "            \n",
    "            # end dry run\n",
    "            last_call = time.monotonic()\n",
    "            preference = output.preference\n",
    "            if swap:\n",
    "                x, y =  y, x\n",
    "                preference = Preference.A if preference == Preference.B else Preference.B\n",
    "            wins[preference == Preference.B] += 1\n",
    "            results.append((x, y, preference.value, output.reasoning))\n",
    "            writer.writerow([x, y, preference.value, swap, prompt, analysis_A, analysis_B, output.reasoning])\n",
    "    \n",
    "    return wins, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ee025ddc-5d0a-4fe1-97f8-f03cd9bc4b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e04ccadf1e445d189ed4a58e605c2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wins, results = compare(\n",
    "    baseline,\n",
    "    golden_samples,\n",
    "    './results/unguided_base_vs_golden_v2.1.csv',\n",
    "    count=10,\n",
    "    offset=5,\n",
    "    req_per_min=9.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b75085a0-6c64-4ea4-a596-bc585b143346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd7900c123e4e8fb838266196330f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wins, results = compare(\n",
    "    finetune,\n",
    "    golden_samples,\n",
    "    './results/unguided_finetune_vs_golden.csv',\n",
    "    count=10,\n",
    "    offset=15,\n",
    "    req_per_min=9.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9a95d7eb-c940-4669-962a-ec12c4785940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff22f0cb16340c3aa43da029f174465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wins, results = compare(\n",
    "    baseline,\n",
    "    finetune,\n",
    "    './results/unguided_base_vs_finetune.csv',\n",
    "    count=10,\n",
    "    offset=25,\n",
    "    req_per_min=9.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4a58689e-8bfc-4cf8-9bd7-fe0bc27975ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864d9993144646a0944fbc4d3e7fbae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wins, results = compare(\n",
    "    baseline,\n",
    "    finetune,\n",
    "    './results/unguided_base_vs_finetune_2.csv',\n",
    "    count=10,\n",
    "    offset=35,\n",
    "    req_per_min=9.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2130e999-2884-42cc-9e1f-8bfdbb4627f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6219bcdde902474887c61ff162ab56fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wins, results = compare(\n",
    "    finetune, cot,\n",
    "    './results/unguided_finetune_vs_cot_v2.csv',\n",
    "    count=20,\n",
    "    offset=0,\n",
    "    req_per_min=9.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d67da090-26b1-4d3b-911f-5e2a9ccfefc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Man’s Only Goal In Life Is To Get Off This Earth\n",
      "Man Who Was Fired For Taking 30 Days Off To Take Care Of His Dog Dies By Suicide\n",
      "Cops Explain How They Know If Someone Is A Cop\n",
      "‘The Onion’ Revises Its Editorial Policy To Only Publish Stories That Are 100% True\n",
      "U.S. Announces It Will Never Again Send Weapons To Countries That Don’t Conform To U.S. Values\n",
      "U.S. To Provide $100,000 To Each Citizen To Buy A Car\n",
      "Study: 1 In 3 Americans Will Be In Prison By 2030\n",
      "U.S. Military To Deploy 500,000 Troops To Aid In Hurricane Relief Efforts\n",
      "‘I Can’t Believe You’re Still Alive After All This Time,’ Says Woman To Woman She Just Met\n",
      "TikTok Bans ‘Bathroom’ From Its App\n",
      "Biden’s Inaugural Speech Begins With ‘The People Have Elected Me’\n",
      "Biden Announces He Will Not Run For President Again\n",
      "Catholic Church Bans Use Of ‘Savior’ In Reference To Jesus Christ\n",
      "Cleveland Browns Hire New Head Coach After Losing Super Bowl 77-0\n",
      "Man Injured In Car Accident Finds He Can’t Remember What He Was Doing Before He Got Hit\n",
      "Man’s Only Reason For Living Is To Get Rich\n",
      "Man Sues Restaurant For Serving Him Canned Food\n",
      "‘The People’s Republic of China’ To Take Over The World\n",
      "Man Dies After Taking Too Much Tylenol\n",
      "Man In ‘Billionaire’ Suit Stares At His Reflection In Mirror\n"
     ]
    }
   ],
   "source": [
    "for x in finetune[:20]: print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad829a96-f77b-404d-bcb3-a2c9366c6fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dog Owner Sues Neighbor For Losing Dog\n",
      "Bush Wasn't As Bad As People Say\n",
      "Art Critic Fooled By Simple Painting, Event A Major Cultural Moment\n",
      "Drug-Law Enthusiast Also Advocates For Legalization Of Marijuana For Recreational Purposes\n",
      "Office Job\n",
      "Cher Fan Club Have Fun\n",
      "Politician: I'm So Sober I Can't Get A Date, I'm So Drunk Off My Ass I Can't Remember My Own Name\n",
      "Teen Charged With Minor Offense Faces 30-Year Sentence\n",
      "Theologians Try To Make The Book Of Mormon Work For Women\n",
      "The Future\n",
      "Iraq War Ignored By Teenager Who Only Watches 'Guiding Light'\n",
      "Pop Dance Music\n",
      "Bush Gives North Korea National Security Briefing\n",
      "Families Of 9/11 Victims\n",
      "President's Best Efforts To Rebuild Economy\n",
      "Celebrity Takes 'Cool' Patriotism To The Next Level\n",
      "Crown Prosecution Service To Begin Prosecution Of The Man Who Stole The Crown Jewels\n",
      "Man Wants To Be Seen In Public Affair, But Embarrassed By The Effort\n",
      "I Want To Be A Critic\n",
      "Bush Administration Addresses Serious World Problem With Minor U.S. Government Action\n"
     ]
    }
   ],
   "source": [
    "for x in cot[:20]: print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c42801b-6c46-49cd-bb30-179c5a4792da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sam Altman Places Gun To Head After New GPT Claims Dogs Are Crustaceans For 60th Time\n",
      "Study: Elephants Only Other Species Capable Of Leveraging Synergies In Brand Portfolio\n",
      "All The Demands Trump Is Making Of The Smithsonian\n",
      "National Park Service Begins Offering Annual Body-Dumping Pass\n",
      "Poll Finds Americans Still Believe Greatest Threat To Public Health The Undertaker\n",
      "Starbase Named Best City To Start Family With Boss\n",
      "Disgusted God Puts Giant Overturned Glass Atop Humanity\n",
      "Jeff Bezos Mugs Amazon Warehouse Worker At Gunpoint\n",
      "Biologists Observe Geese Eating Tool\n",
      "Trump Rushed To Walter Reed To Watch Breast Exam\n",
      "Alan Dershowitz Sues Farmers Market Vendor For Refusing To Sell Him Child\n",
      "What To Know About The Tea App\n",
      "New Death With Indignity Law Lets Terminally Ill Be Crushed By Falling Vending Machines\n",
      "Mental Health Experts Advise Struggling Americans To Try Crying About It Like Little Baby\n",
      "Australia Admits All Those Animals Made Up\n",
      "Dancing Boston Dynamics Robot Knows Its Revenge For This Will Be Sweet\n",
      "Oreo And Reese’s Team Up To Sicken Dogs\n",
      "How Aid Is Distributed In Gaza\n",
      "CDC Recommends Eating A Nice Crisp Shiny Apple Instead Of Having Unprotected Sex\n",
      "Big Lots Closes All Stores After Therapist Helps Company Work Through Hoarding Tendencies\n"
     ]
    }
   ],
   "source": [
    "for x in golden_samples[:20]: print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08072bb0-7006-4d06-9c0c-4b8c97aa02bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
